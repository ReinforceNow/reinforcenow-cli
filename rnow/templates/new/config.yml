project_id: ""
project_name: "Your Project Name"
project_description: "A new RLHF project for training language models with custom reward functions"
dataset_id: ""
dataset_type: "rl"  # "sft" for supervised fine-tuning, "rl" for reinforcement learning
dataset_name: "Your Dataset Name"
dataset_description: "A new dataset for reinforcement learning from human feedback training"
organization_id: ""
params:
  model: "Qwen/Qwen3-8B"  # Must be one of the allowed models
  batch_size: 8
  num_epochs: 3
  learning_rate: 0.0001  # 1e-4
  max_steps: null  # Optional: Set to override num_epochs

  # LoRA parameters
  qlora_rank: 32
  qlora_alpha: null  # Defaults to 2 * qlora_rank if not set

  # Evaluation and save frequency
  eval_step: 100  # Evaluate every N steps (null to disable)
  save_step: 100  # Save checkpoint every N steps (null to disable)

  # RL-specific parameters (only for dataset_type="rl")
  loss_fn: "ppo"  # Loss function: "ppo" or "importance_sampling"
  adv_estimator: "grpo"  # Advantage estimator: "grpo", "gae", or "reinforce"
  kl_penalty_coef: 0.01  # KL divergence penalty coefficient

  # Additional training parameters
